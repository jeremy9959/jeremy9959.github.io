<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Gradient Descent</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 data-number="1" id="gradient-descent"><span class="header-section-number">1</span> Gradient Descent</h1>
<h2 data-number="1.1" id="motivation"><span class="header-section-number">1.1</span> Motivation</h2>
<p>In Linear Regression, we studied how to minimize the error function <span class="math display">\[ E = \sum_{j=1}^{N} (y_j-\sum_{s=1}^{k+1} x_{js}m_{s})^2 \]</span> and obtained an exact solution. In other cases we will encounter later, such an exact solution is not feasible and we will have to use a method to approximate an exact solution. One of the most common methods for this purpose is <em>gradient descent</em>.</p>
<h2 data-number="1.2" id="basic-algorithm"><span class="header-section-number">1.2</span> Basic Algorithm</h2>
<p>Consider a function <span class="math inline">\(E: \mathbb R^n \rightarrow \mathbb R\)</span>, <span class="math inline">\({\boldsymbol{w}}=(w_1, w_2, \dots , w_n) \rightarrow E({\boldsymbol{w}})\)</span>. The <em>gradient</em> <span class="math inline">\(\nabla E\)</span> of <span class="math inline">\(E\)</span> is defined by <span class="math display">\[  \nabla E := \left ( \frac {\partial E}{\partial w_1}, \frac {\partial E}{\partial w_2}, \dots , \frac {\partial E}{\partial w_n} \right ). \]</span></p>
<p><strong>Proposition </strong>: <em>Assume that <span class="math inline">\(E({\boldsymbol{w}})\)</span> is differentiable in a neighborhood of <span class="math inline">\({\boldsymbol{w}}\)</span>. Then the function <span class="math inline">\(E({\boldsymbol{w}})\)</span> decreases fastest in the direction of <span class="math inline">\(-\nabla E ({\boldsymbol{w}})\)</span>.</em></p>
<p><strong>Proof:</strong> For a unit vector <span class="math inline">\({\boldsymbol{u}}\)</span>, the directional derivative <span class="math inline">\(D_{\boldsymbol{u}}E\)</span> is given by <span class="math display">\[ D_{\boldsymbol{u}}E= \nabla E \cdot {\boldsymbol{u}}= | \nabla E | |{\boldsymbol{u}}| \cos \theta = | \nabla E | \cos \theta ,\]</span> where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(\nabla E\)</span> and <span class="math inline">\({\boldsymbol{u}}\)</span>. The minimum value of <span class="math inline">\(D_{\boldsymbol{u}}E\)</span> occurs when <span class="math inline">\(\cos \theta\)</span> is <span class="math inline">\(-1\)</span>.     <span class="math inline">\(\square\)</span></p>
<p>Set <span class="math display">\[ {\boldsymbol{w}}_{k+1}={\boldsymbol{w}}_k - \eta \nabla E({\boldsymbol{w}}_k) \]</span> where <span class="math inline">\(\eta &gt;0\)</span> is the step size or <em>learning rate</em>. Then <span class="math display">\[ E({\boldsymbol{w}}_{k+1}) \le E({\boldsymbol{w}}_{k}).\]</span> Under some moderate conditions, <span class="math display">\[ E({\boldsymbol{w}}_k) \rightarrow \text{local minimum} \qquad \text{ as }\ k \rightarrow \infty .\]</span>In particular, this is true when <span class="math inline">\(E\)</span> is convex or when <span class="math inline">\(\nabla E\)</span> is Lipschitz continuous.</p>
<h3 data-number="1.2.1" id="example"><span class="header-section-number">1.2.1</span> Example</h3>
<p>Consider <span class="math inline">\(E({\boldsymbol{w}})=E(w_1,w_2)= w_1^4+w_2^4-16w_1w_2\)</span>. Then <span class="math inline">\(\nabla E({\boldsymbol{w}})= [ 4w_1^3-16w_2, 4w_2^3-16w_1]\)</span>. Choose <span class="math inline">\({\boldsymbol{w}}_0=(1,1)\)</span> and <span class="math inline">\(\eta =0.01\)</span>. Then <span class="math display">\[ {\boldsymbol{w}}_{30}=(1.99995558586289, 1.99995558586289)\]</span> and we get <span class="math display">\[ E({\boldsymbol{w}}_{30})= -31.9999999368777. \]</span></p>
<p>We see that <span class="math inline">\({\boldsymbol{w}}_k \rightarrow (2,2)\)</span> and <span class="math inline">\(E(2,2)=-32\)</span>. Indeed, using multi-variable calculus, one can verify that when <span class="math inline">\({\boldsymbol{w}}=(2,2)\)</span>, a local minimum of <span class="math inline">\(E({\boldsymbol{w}})\)</span> is <span class="math inline">\(-32\)</span>.</p>
<p><strong>Exercise</strong>: Using multi-variable calculus, find all the local minima of <span class="math inline">\(E({\boldsymbol{w}})\)</span>.</p>
<div id="fig:des" class="fignos">
<figure>
<img src="descent.png" style="width:30.0%" alt="" /><figcaption><span>Figure 1:</span> Graph of <span class="math inline">\(E({\boldsymbol{w}})\)</span></figcaption>
</figure>
</div>
<div id="fig:tab" class="fignos">
<figure>
<img src="table.png" style="width:50.0%" alt="" /><figcaption><span>Figure 2:</span> Values of <span class="math inline">\(E({\boldsymbol{w}}_k)\)</span></figcaption>
</figure>
</div>
<h3 data-number="1.2.2" id="example-linear-regression-revisited"><span class="header-section-number">1.2.2</span> Example: Linear Regression revisited</h3>
<p>We will apply gradient descent to linear regression in the lab session.</p>
<p><strong>Exercise</strong>: Define <span class="math inline">\(\sigma(x) = \dfrac {e^x}{e^x+1}= \dfrac 1 {1+e^{-x}}\)</span>. In Logistic Regression we will minimize the following error function <span class="math display">\[ E ({\boldsymbol{w}}) = - \sum_{n=1}^N \{ t_n \ln y_n + (1-t_n) \ln (1-y_n)\},  \]</span> where we write <span class="math inline">\({\boldsymbol{w}}=(w_1, w_2, \dots , w_{k+1})\)</span> and <span class="math inline">\(y_n=\sigma(w_1 x_{n1}+ w_2 x_{n2} + \cdots + w_k x_{nk}+w_{k+1} )\)</span>. Compute the gradient <span class="math inline">\(\nabla E({\boldsymbol{w}})\)</span>.</p>
<h2 data-number="1.3" id="newtons-method"><span class="header-section-number">1.3</span> Newton’s Method</h2>
<p>Let us first consider the single-variable case.</p>
<ul>
<li><p>Let <span class="math inline">\(f: \mathbb R \longrightarrow \mathbb R\)</span> be a single-variable (convex, differentiable) function.</p></li>
<li><p>To find a local minimum <span class="math inline">\(\Longleftrightarrow\)</span> To find <span class="math inline">\(x^*\)</span> such that <span class="math inline">\(f&#39;(x^*)=0\)</span><br />
Make a guess <span class="math inline">\(x_0\)</span> for <span class="math inline">\(x^*\)</span> and set <span class="math inline">\(x=x_0+h\)</span>.</p></li>
<li><p>Using the Taylor expansion, we have <span class="math display">\[\begin{align*} f(x) &amp;=f(x_0+h) \approx f(x_0) + f&#39;(x_0) h+ \tfrac 1 2 f&#39;&#39;(x_0) h^2 \\ f&#39;(x) = \frac{df}{dh}\frac {dh}{dx} &amp; \approx \frac d {dh} \left ( f(x_0) + f&#39;(x_0) h+ \tfrac 1 2 f&#39;&#39;(x_0) h^2 \right ) \\
&amp;= f&#39;(x_0) + f&#39;&#39;(x_0)h \end{align*}\]</span> If <span class="math inline">\(0 = f&#39;(x_0) + f&#39;&#39;(x_0)h\)</span>, we obtain <span class="math display">\[ h = - f&#39;(x_0)/f&#39;&#39;(x_0). \]</span></p></li>
<li><p>We have shown that <span class="math display">\[ x_1=x_0- f&#39;(x_0)/f&#39;&#39;(x_0) \]</span> is an approximation of <span class="math inline">\(x^*\)</span>.</p></li>
<li><p>Repeat the process to obtain <span class="math display">\[ \boxed{x_{k+1}=x_k- f&#39;(x_k)/f&#39;&#39;(x_k)},  \]</span> and <span class="math inline">\(x_k \rightarrow x^*\)</span> as <span class="math inline">\(k \rightarrow \infty\)</span>.</p></li>
</ul>
<p>Now we consider the multi-variable case.</p>
<ul>
<li><p>Let <span class="math inline">\(E({\boldsymbol{w}})\)</span> be a multi-variable function. The <em>Hessian matrix</em> of <span class="math inline">\(E\)</span> is defined by <span class="math display">\[ \mathbf H E= \begin{bmatrix} \tfrac {\partial^2 E}{\partial w_1^2} &amp;  \tfrac {\partial^2 E}{\partial w_1 \partial w_2} &amp; \cdots &amp;  \tfrac {\partial^2 E}{\partial w_1 \partial w_n} \\ \tfrac {\partial^2 E}{\partial w_2 \partial w_1} &amp; \tfrac {\partial^2 E}{\partial w_2^2} &amp; \cdots &amp; \tfrac {\partial^2 E}{\partial w_2 \partial w_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \tfrac {\partial^2 E}{\partial w_n \partial w_1} &amp; \tfrac {\partial^2 E}{\partial w_n \partial w_2} &amp; \cdots &amp; \tfrac {\partial^2 E}{\partial w_n^2}  \end{bmatrix}. \]</span> That is, <span class="math inline">\(\mathbf H E= [ \tfrac {\partial^2 E}{\partial w_i \partial w_j}]\)</span>.</p></li>
<li><p>Generalizing the single-variable case, we obtain<br />
<span class="math display">\[ \boxed{ {\boldsymbol{w}}_{k+1}= {\boldsymbol{w}}_k - \mathbf H E ({\boldsymbol{w}}_{k})^{-1} \nabla E({\boldsymbol{w}}_k)} . \]</span></p></li>
<li><p>Using a step size <span class="math inline">\(\eta\)</span>, the formula may be modified to be <span class="math display">\[  \boxed{ {\boldsymbol{w}}_{k+1}= {\boldsymbol{w}}_k - \eta \mathbf H E ({\boldsymbol{w}}_{k})^{-1} \nabla E({\boldsymbol{w}}_k)} . \]</span></p></li>
<li><p>Newton’s method is much faster than Gradient Descent. However, it may be expensive to compute the inverse of the Hessian matrix.</p></li>
</ul>
<h3 data-number="1.3.1" id="example-1"><span class="header-section-number">1.3.1</span> Example</h3>
<ul>
<li>Consider <span class="math inline">\(E({\boldsymbol{w}})=E(w_1,w_2)= w_1^4+w_2^4-16w_1w_2\)</span>. Then <span class="math inline">\(\nabla E({\boldsymbol{w}})= [ 4w_1^3-16w_2, 4w_2^3-16w_1]^\top\)</span>.</li>
</ul>
<p><span class="math display">\[\begin{align*} \mathbf H E({\boldsymbol{w}}) &amp;= \begin{bmatrix} 12 w_1^2 &amp; -16 \\ -16 &amp; 12 w_2^2 \end{bmatrix} \\ \\ \mathbf H E({\boldsymbol{w}})^{-1} &amp;= \frac 1 {9w_1^2 w_2^2 -16} \begin{bmatrix} \tfrac 3 4 w_2^2 &amp; 1 \\ 1&amp; \frac 3 4 w_1^2  \end{bmatrix} \\  \\
\mathbf H E^{-1} \nabla E &amp; = \frac 1 {9w_1^2 w_2^2 -16} \begin{bmatrix} 3 w_1^3 w_2^2 -8 w_2^3 -16w_1 \\ 3 w_1^2 w_2^3 -8 w_1^3  -16w_2  \end{bmatrix} \end{align*}\]</span></p>
<!---
- Choose
$\bw_0=(1,1)$ and
$\eta =1$.
Then $\bw_{1}=(2,2)$
 and $E(\bw_{1})= -32$.
-->
<ul>
<li>Choose <span class="math inline">\({\boldsymbol{w}}_0=(1.2,1.2)\)</span> and <span class="math inline">\(\eta =1\)</span>. Then <span class="math inline">\({\boldsymbol{w}}_{9}=(2.00000004189571, 2.00000004189571)\)</span>, <span class="math inline">\(E({\boldsymbol{w}}_{9})= -31.9999999999999\)</span>.</li>
</ul>
<div id="fig:tab" class="fignos">
<figure>
<img src="table-1.png" style="width:50.0%" alt="" /><figcaption><span>Figure 3:</span> Values of <span class="math inline">\(E({\boldsymbol{w}}_k)\)</span></figcaption>
</figure>
</div>
<h3 data-number="1.3.2" id="stochastic-gradient-descent-sgd"><span class="header-section-number">1.3.2</span> Stochastic Gradient Descent (SGD)</h3>
<p>Typically in Machine Learning, the function <span class="math inline">\(E({\boldsymbol{w}})\)</span> is given by a sum of the form <span class="math display">\[ E({\boldsymbol{w}}) = \frac 1 N \sum_{n=1}^N E_n({\boldsymbol{w}}), \]</span> where <span class="math inline">\(N\)</span> is the number of elements in the training set. When <span class="math inline">\(N\)</span> is large, computation of the gradient <span class="math inline">\(\nabla E\)</span> may be expensive.</p>
<p>The SGD selects a sample from the training set in each iteration step instead of using the whole batch of the training set, and use <span class="math display">\[ \frac 1 M \sum_{i=1}^M \nabla E_{n_i}({\boldsymbol{w}}) ,\]</span> where <span class="math inline">\(M\)</span> is the size of the sample and <span class="math inline">\(\{ n_1, n_2 , \dots , n_M \} \subset \{ 1,2, \dots , N \}\)</span>. The SGD is commonly used in many Machine Learning algorithms.</p>
<p><br><br><br></p>
</body>
</html>
