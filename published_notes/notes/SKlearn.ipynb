{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adverse-index",
   "metadata": {},
   "source": [
    "# The scikit learn library\n",
    "\n",
    "The scikit-learn library is a large open-source python package that carries out a wide range of machine learning algorithms. It has a particular structure,\n",
    "and once you understand that basic structure you can use the documentation to learn how to carry out nearly any algorithm.  In this overview we will look at this structure for Linear Regression and the Naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-accounting",
   "metadata": {},
   "source": [
    "## General Overview\n",
    "\n",
    "Typically, to apply the scikit learn library to a machine learning problem,\n",
    "you\n",
    "\n",
    "1. **construct** an object that represents the algorithm you plan to apply (such as linear regression), setting whatever parameters you choose to control the details of the algorithm.\n",
    "\n",
    "2.  **fit** your training data using the ```fit``` method of your object, which computes the parameters of the algorithm from the data.\n",
    "\n",
    "3. **transform** or **predict** from other data to compute the predicted values based on the fitted model.\n",
    "\n",
    "4. **score** your results or evaluate them in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-reggae",
   "metadata": {},
   "source": [
    "## Illustration using Linear Regression\n",
    "\n",
    "Let's look at how sklearn's linear regression fits this approach.  It's always useful to have the documentation page for the method open so you can see your parameter options.  In this case, we refer to the [documentation for linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "We will begin by generating some simulated data using the statistical model\n",
    "$$\n",
    "y=m_1x_1+m_2x_2+e\n",
    "$$\n",
    "where $e$ is a  normal variable with mean zero and standard deviation 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.uniform(-5,5,size=20)\n",
    "x2 = np.random.uniform(-5,5,size=20)\n",
    "X = np.stack([x1,x2],axis=1)\n",
    "print('Stacking x1 and x2 along axis=1 creates a matrix of shape ',X.shape)\n",
    "y = 3*x1-5*x2+np.random.normal(0,5,size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-mayor",
   "metadata": {},
   "source": [
    "### Step 0. Loading the library\n",
    "\n",
    "Using google, we find that linear regression is part of the linear_model\n",
    "submodule of the vast sklearn library, so we import that module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-backing",
   "metadata": {},
   "source": [
    "### Step 1. Creating the Linear Regression object\n",
    "\n",
    "Here we create a default linear regression object.  Linear regression\n",
    "is a simple algorithm and there are only a few options to this construction;\n",
    "for example, we could say ```fit_intercept=False``` if our data was centered and we didn't want the algorithm to add a column of ones to compute the intercept.  In our case, we will leave the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-cardiff",
   "metadata": {},
   "source": [
    "### Step 2. Fitting the data\n",
    "\n",
    "The object L has a fit method that computes the parameters of the model -- the slopes and intercept.  It takes the data X and the target variable y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = L.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-spokesman",
   "metadata": {},
   "source": [
    "From the object A, we can extract the ```attributes``` or parameters of the model.  The ```coef_``` attribute are the coefficients (M) and the ```intercept_``` attribute is the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-triumph",
   "metadata": {},
   "source": [
    "### Step 3. Making predictions.\n",
    "\n",
    "Now suppose we want to evaluate our linear model on some data.  For example, we want to compute the predicted values yhat from our initial data.  The ```predict``` method does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = L.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=figure(title = 'y vs yhat')\n",
    "f.scatter(x=y,y=yhat)\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-carol",
   "metadata": {},
   "source": [
    "### Step 4. Scoring\n",
    "\n",
    "The linear regression object in sklearn offers a score method that returns the *coefficient of determination* of the model.  This is \n",
    "$$\n",
    "(1-MSE/\\sigma_{y}^2)\n",
    "$$\n",
    "so it compares the mean squared error to the inherent variance in the y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-forest",
   "metadata": {},
   "source": [
    "## Illustration of word counting\n",
    "\n",
    "In the discussion of the \"Naive Bayes\" method, we are faced with the problem of converting text into a vector of word counts.  Scikit learn includes\n",
    "a method for this, which we will now illustrate.  The relevant tool\n",
    "is called ```CountVectorizer.```\n",
    "\n",
    "[It's a good idea to refer to the relevant documentation here.](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "We also need some data, and according the documentation, the countvectorizer object can take input from a filename, a python file, or a sequence of strings.\n",
    "For simplicity we'll work with a few basic sentences.  To get an apostrophe into a string, I escape it and write it as ```\\'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['She\\'s got diamonds on the soles of her shoes','Lucy in the sky with diamonds',\n",
    "        'Blue suede shoes','Oceans of diamonds']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-constraint",
   "metadata": {},
   "source": [
    "### Step 0. Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-nirvana",
   "metadata": {},
   "source": [
    "### Step 1. Create the object\n",
    "\n",
    "The countvectorizer object has many options.  Usually, the defaults are fine, but let's take note of a few interesting\n",
    "ones:\n",
    "    \n",
    "- Setting ```max_features``` to n limits the code to finding the n most common words in the text.\n",
    "- Setting ```stop_words='english'``` tells the code to ignore a list of common words like \"I\", \"at\", and so on. You  can also supply your own list of stop words as an option.\n",
    "- The ```min_df``` and ```max_df``` flags allow you to screen out words whose document frequency is above or below a threshold, so for example if you have a 100 documents you can ignore words that occur in fewer than 20.\n",
    "- The ```binary=True``` option tells the object to only determine if a word is present, rather than to count the occurrences; so it will set all non-zero counts to 1. \n",
    "    \n",
    "The vectorizer can also be given an explicit vocabulary to work with, can be told to look at character sequences instead of words, and can be asked to consider n-grams - groups of n words. \n",
    "\n",
    "We'll just use the default version here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-operation",
   "metadata": {},
   "source": [
    "### Step 2. Fit the data\n",
    "\n",
    "We fit the data, creating an object A.  From that object, we can extract the words the vectorizer found.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = V.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-optimum",
   "metadata": {},
   "source": [
    "A.vocabulary is a python dictionary that associates each word in the vocabulary with a number.  That number is an index \n",
    "so that when we create a matrix of documents by word counts, we can associate columns to keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-motel",
   "metadata": {},
   "source": [
    "The get_feature_names() method returns a list of the vocabulary words in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-blade",
   "metadata": {},
   "source": [
    "### Step 3. Transforming data\n",
    "\n",
    "Now we can use the fitted object to convert text to a matrix.  The result is a sparse matrix, which\n",
    "is a way of storing the data efficiently since these matrices can be quite large.  The toarray method gives us a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = A.transform(data)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-husband",
   "metadata": {},
   "source": [
    "In the matrix above, each row corresponds to one of our original sentences, and each column to a keyword. Refering\n",
    "back to the vocabulary index, we see that 'of' is column 7, and looking at column 7 of the matrix we see that\n",
    "'of' occurs in the first and last sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-handling",
   "metadata": {},
   "source": [
    "If we want the total occurrences of each word in the data, we can sum by columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-briefing",
   "metadata": {},
   "source": [
    "Finally, we can compute word occurrences **using the derived vocabulary** by calling transform on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=A.transform(['Here is a new sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-dispatch",
   "metadata": {},
   "source": [
    "Notice that Y is all zeros -- that's because none of the words in that sentence are in the vocabulary that the object computed from the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-partnership",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
